{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amelft81/EEGASD/blob/main/complete_eeg_to_asd_prediction_pipeline__simulated_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import models, transforms\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import os\n",
        "import copy # For deep copying model state for early stopping\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.interpolate import griddata\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "\n",
        "# --- 1. Configuration Parameters ---\n",
        "# Data generation parameters (from eeg_dataset_generator)\n",
        "NUM_ELECTRODES = 116 # Paper mentions 116 electrodes after exclusion\n",
        "SAMPLING_RATE = 500  # Hz, as per paper\n",
        "SEGMENT_LENGTH_SEC = 1 # seconds, as per paper\n",
        "NUM_SAMPLES_PER_SEGMENT = SAMPLING_RATE * SEGMENT_LENGTH_SEC # 500 samples\n",
        "IMAGE_SIZE = 224     # 224x224 pixels, as per ResNet-50 input\n",
        "FREQ_BANDS = {\n",
        "    'theta': (4, 7),   # Hz\n",
        "    'alpha': (8, 13),  # Hz\n",
        "    'beta': (13, 30)   # Hz\n",
        "}\n",
        "# Desired initial class distribution (approximate from paper: 81% NON-ASD, 19% ASD)\n",
        "ASD_RATIO = 0.19\n",
        "INITIAL_TOTAL_SAMPLES = 1000 # Total samples to generate before oversampling (for demonstration)\n",
        "\n",
        "# Model and training parameters (from resnet_eeg_classifier)\n",
        "NUM_CLASSES = 2 # ASD (1) or NON-ASD (0)\n",
        "BATCH_SIZE = 100 # As per paper\n",
        "LEARNING_RATE = 1e-3 # As per paper\n",
        "NUM_EPOCHS = 100 # Maximum epochs, early stopping will likely stop sooner\n",
        "PATIENCE = 10 # Number of epochs to wait for improvement before stopping (for Early Stopping)\n",
        "\n",
        "# ImageNet normalization values for pre-trained models\n",
        "NORM_MEAN = [0.485, 0.456, 0.406]\n",
        "NORM_STD = [0.229, 0.224, 0.225]\n",
        "\n",
        "# Set device to GPU if available, otherwise CPU\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "# --- 2. Simulated EEG Data Generation and Image Transformation Functions ---\n",
        "\n",
        "def generate_electrode_positions(num_electrodes, radius=0.5):\n",
        "    \"\"\"Generates simulated 2D electrode positions in a circular pattern.\"\"\"\n",
        "    angles = np.linspace(0, 2 * np.pi, num_electrodes, endpoint=False)\n",
        "    x = radius * np.cos(angles) + np.random.normal(0, 0.05, num_electrodes) # Add some noise\n",
        "    y = radius * np.sin(angles) + np.random.normal(0, 0.05, num_electrodes) # Add some noise\n",
        "    # Add a central electrode\n",
        "    x = np.append(x, 0)\n",
        "    y = np.append(y, 0)\n",
        "    return np.array([x, y]).T\n",
        "\n",
        "def generate_eeg_signal(num_samples, sampling_rate, freq_bands, is_asd=False):\n",
        "    \"\"\"\n",
        "    Generates a simulated EEG signal for one electrode.\n",
        "    ASD signals might have slightly different characteristics (e.g., more noise or altered band power).\n",
        "    \"\"\"\n",
        "    t = np.linspace(0, num_samples / sampling_rate, num_samples, endpoint=False)\n",
        "    signal = np.zeros(num_samples)\n",
        "\n",
        "    # Add base frequencies for each band\n",
        "    for band_name, (low_f, high_f) in freq_bands.items():\n",
        "        center_f = (low_f + high_f) / 2\n",
        "        amplitude = np.random.uniform(0.5, 1.5)\n",
        "        # Simplified simulation: ASD signals might have altered power in certain bands\n",
        "        if is_asd:\n",
        "            if band_name == 'theta': # Example: slightly higher theta in ASD\n",
        "                amplitude *= 1.2\n",
        "            elif band_name == 'beta': # Example: slightly lower beta in ASD\n",
        "                amplitude *= 0.8\n",
        "        signal += amplitude * np.sin(2 * np.pi * center_f * t + np.random.uniform(0, 2 * np.pi))\n",
        "\n",
        "    # Add random noise\n",
        "    noise_level = np.random.uniform(0.1, 0.5)\n",
        "    if is_asd: # Simplified simulation: ASD signals might be noisier\n",
        "        noise_level *= 1.2\n",
        "    signal += noise_level * np.random.randn(num_samples)\n",
        "\n",
        "    return signal\n",
        "\n",
        "def get_band_power(signal, sampling_rate, freq_range):\n",
        "    \"\"\"Calculates the average power in a specific frequency range using FFT.\"\"\"\n",
        "    n = len(signal)\n",
        "    yf = np.fft.fft(signal)\n",
        "    xf = np.fft.fftfreq(n, 1 / sampling_rate)\n",
        "\n",
        "    # Find indices corresponding to the frequency range\n",
        "    min_freq, max_freq = freq_range\n",
        "    indices = np.where((xf >= min_freq) & (xf <= max_freq))\n",
        "\n",
        "    # Calculate power (magnitude squared) in the specified band\n",
        "    power = np.mean(np.abs(yf[indices])**2)\n",
        "    return power\n",
        "\n",
        "def normalize_band_to_uint8(arr):\n",
        "    \"\"\"\n",
        "    Normalizes a 2D array to the 0-255 range and converts it to uint8.\n",
        "    Handles cases where min/max might be the same (flat array).\n",
        "    \"\"\"\n",
        "    min_val = np.min(arr)\n",
        "    max_val = np.max(arr)\n",
        "    if max_val == min_val:\n",
        "        return np.full(arr.shape, 128, dtype=np.uint8) # Default grey if flat\n",
        "    return ((arr - min_val) / (max_val - min_val) * 255).astype(np.uint8)\n",
        "\n",
        "def create_eeg_image(electrode_positions, band_powers, image_size):\n",
        "    \"\"\"\n",
        "    Interpolates band powers onto a 2D grid and creates an RGB image.\n",
        "    Each channel (R, G, B) corresponds to a frequency band.\n",
        "    \"\"\"\n",
        "    grid_x, grid_y = np.mgrid[-1:1:complex(0, image_size), -1:1:complex(0, image_size)]\n",
        "\n",
        "    # Interpolate for each band\n",
        "    # Method='cubic' for smoother interpolation, fill_value=0 for outside points\n",
        "    interp_theta = griddata(electrode_positions, band_powers['theta'], (grid_x, grid_y), method='cubic', fill_value=0)\n",
        "    interp_alpha = griddata(electrode_positions, band_powers['alpha'], (grid_x, grid_y), method='cubic', fill_value=0)\n",
        "    interp_beta = griddata(electrode_positions, band_powers['beta'], (grid_x, grid_y), method='cubic', fill_value=0)\n",
        "\n",
        "    # Assign bands to RGB channels and normalize\n",
        "    # Theta -> Red, Alpha -> Green, Beta -> Blue (common choice, can be varied)\n",
        "    img_r = normalize_band_to_uint8(interp_theta)\n",
        "    img_g = normalize_band_to_uint8(interp_alpha)\n",
        "    img_b = normalize_band_to_uint8(interp_beta)\n",
        "\n",
        "    # Stack into an RGB image (H, W, C)\n",
        "    eeg_image = np.stack([img_r, img_g, img_b], axis=-1)\n",
        "    return eeg_image\n",
        "\n",
        "def generate_and_oversample_dataset(num_samples_total, asd_ratio, image_size, num_electrodes, sampling_rate, freq_bands):\n",
        "    \"\"\"\n",
        "    Generates a synthetic EEG image dataset and then oversamples the minority class.\n",
        "    Returns X (images) and y (labels) as numpy arrays.\n",
        "    \"\"\"\n",
        "    dataset = []\n",
        "    electrode_positions = generate_electrode_positions(num_electrodes)\n",
        "\n",
        "    print(f\"Generating {num_samples_total} initial synthetic EEG images...\")\n",
        "    for i in range(num_samples_total):\n",
        "        is_asd = np.random.rand() < asd_ratio\n",
        "        label = 1 if is_asd else 0 # 1 for ASD, 0 for NON-ASD\n",
        "\n",
        "        # Generate EEG signals for all electrodes\n",
        "        # +1 because generate_electrode_positions adds a central electrode\n",
        "        electrode_signals = [generate_eeg_signal(NUM_SAMPLES_PER_SEGMENT, sampling_rate, freq_bands, is_asd)\n",
        "                             for _ in range(num_electrodes + 1)]\n",
        "\n",
        "        # Calculate band powers for each electrode\n",
        "        band_powers_per_electrode = {band: [] for band in freq_bands}\n",
        "        for signal in electrode_signals:\n",
        "            for band_name, freq_range in freq_bands.items():\n",
        "                power = get_band_power(signal, sampling_rate, freq_range)\n",
        "                band_powers_per_electrode[band_name].append(power)\n",
        "\n",
        "        # Create the EEG image\n",
        "        eeg_image = create_eeg_image(electrode_positions, band_powers_per_electrode, image_size)\n",
        "        dataset.append((eeg_image, label))\n",
        "\n",
        "        if (i + 1) % (num_samples_total // 10) == 0:\n",
        "            print(f\"  Generated {i + 1}/{num_samples_total} samples...\")\n",
        "\n",
        "    print(\"Initial dataset generation complete.\")\n",
        "\n",
        "    X = np.array([item[0] for item in dataset])\n",
        "    y = np.array([item[1] for item in dataset])\n",
        "\n",
        "    print(f\"Original dataset shape: {Counter(y)}\")\n",
        "\n",
        "    # Apply RandomOverSampler to balance the dataset\n",
        "    print(\"Applying RandomOverSampler to balance the dataset...\")\n",
        "    n_samples, h, w, c = X.shape\n",
        "    X_flat = X.reshape(n_samples, -1) # Flatten images for the sampler\n",
        "\n",
        "    from imblearn.over_sampling import RandomOverSampler\n",
        "    ros = RandomOverSampler(random_state=42)\n",
        "    X_resampled_flat, y_resampled = ros.fit_resample(X_flat, y)\n",
        "\n",
        "    X_resampled = X_resampled_flat.reshape(-1, h, w, c) # Reshape images back\n",
        "    print(f\"Resampled dataset shape: {Counter(y_resampled)}\")\n",
        "    print(\"Oversampling complete.\")\n",
        "\n",
        "    return X_resampled, y_resampled\n",
        "\n",
        "# --- 3. Custom PyTorch Dataset Class ---\n",
        "class EEGImageDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Custom PyTorch Dataset for loading EEG images and their labels.\n",
        "    \"\"\"\n",
        "    def __init__(self, images, labels, transform=None):\n",
        "        self.images = images\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = self.images[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "# --- 4. Model Definition: Pre-trained ResNet-50 ---\n",
        "def get_resnet_model(num_classes, freeze_features=True):\n",
        "    \"\"\"\n",
        "    Loads a pre-trained ResNet-50 model and modifies its final layer\n",
        "    for binary classification.\n",
        "    \"\"\"\n",
        "    model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n",
        "    print(\"Loaded pre-trained ResNet-50 model.\")\n",
        "\n",
        "    if freeze_features:\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = False\n",
        "        print(\"Frozen all feature extractor layers.\")\n",
        "\n",
        "    num_ftrs = model.fc.in_features\n",
        "    model.fc = nn.Linear(num_ftrs, num_classes)\n",
        "    print(f\"Modified final fully connected layer to output {num_classes} classes.\")\n",
        "\n",
        "    return model\n",
        "\n",
        "# --- 5. Data Loading and Preprocessing (using generated data) ---\n",
        "def prepare_data_loaders(images, labels, batch_size, norm_mean, norm_std):\n",
        "    \"\"\"\n",
        "    Prepares PyTorch DataLoaders from the generated and oversampled dataset.\n",
        "    \"\"\"\n",
        "    data_transforms = transforms.Compose([\n",
        "        transforms.ToPILImage(), # Convert numpy array to PIL Image for torchvision transforms\n",
        "        transforms.ToTensor(),   # Converts PIL Image to FloatTensor (0-1) and (C, H, W)\n",
        "        transforms.Normalize(mean=norm_mean, std=norm_std) # Normalize with ImageNet stats\n",
        "    ])\n",
        "\n",
        "    # Split data into training and validation sets (80/20 split as in paper's experiments)\n",
        "    train_images, val_images, train_labels, val_labels = train_test_split(\n",
        "        images, labels, test_size=0.2, random_state=42, stratify=labels\n",
        "    )\n",
        "\n",
        "    print(f\"\\nTraining set distribution after split: {Counter(train_labels)}\")\n",
        "    print(f\"Validation set distribution after split: {Counter(val_labels)}\")\n",
        "\n",
        "    # Create datasets\n",
        "    train_dataset = EEGImageDataset(train_images, train_labels, transform=data_transforms)\n",
        "    val_dataset = EEGImageDataset(val_images, val_labels, transform=data_transforms)\n",
        "\n",
        "    # Weighted Random Sampler for training data (already oversampled, but sampler ensures balanced batches)\n",
        "    # The oversampling already balanced the dataset, so weights here will be uniform if `num_samples` is the new total.\n",
        "    # However, the paper explicitly mentions WRS with replacement for balanced mini-batches,\n",
        "    # so we'll re-calculate weights based on the (now balanced) train_labels.\n",
        "    class_counts = Counter(train_labels)\n",
        "    num_samples_train = sum(class_counts.values())\n",
        "    class_weights = {cls: num_samples_train / count for cls, count in class_counts.items()}\n",
        "    sample_weights = [class_weights[label] for label in train_labels]\n",
        "    sampler = WeightedRandomSampler(\n",
        "        weights=sample_weights,\n",
        "        num_samples=num_samples_train, # Draw 'num_samples_train' times (with replacement)\n",
        "        replacement=True\n",
        "    )\n",
        "    print(\"WeightedRandomSampler initialized for training data loaders.\")\n",
        "\n",
        "    # Create DataLoaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=sampler, num_workers=0)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
        "\n",
        "    return train_loader, val_loader\n",
        "\n",
        "# --- 6. Training Function with Early Stopping ---\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, patience, device):\n",
        "    \"\"\"\n",
        "    Trains the deep learning model with early stopping.\n",
        "    \"\"\"\n",
        "    best_val_loss = float('inf')\n",
        "    epochs_no_improve = 0\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    print(\"\\nStarting model training...\")\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_train_loss = 0.0\n",
        "        correct_train_predictions = 0\n",
        "        total_train_samples = 0\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_train_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total_train_samples += labels.size(0)\n",
        "            correct_train_predictions += (predicted == labels).sum().item()\n",
        "\n",
        "        epoch_train_loss = running_train_loss / total_train_samples\n",
        "        epoch_train_accuracy = correct_train_predictions / total_train_samples * 100\n",
        "\n",
        "        # --- Validation Phase ---\n",
        "        model.eval()\n",
        "        running_val_loss = 0.0\n",
        "        correct_val_predictions = 0\n",
        "        total_val_samples = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                running_val_loss += loss.item() * inputs.size(0)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total_val_samples += labels.size(0)\n",
        "                correct_val_predictions += (predicted == labels).sum().item()\n",
        "\n",
        "        epoch_val_loss = running_val_loss / total_val_samples\n",
        "        epoch_val_accuracy = correct_val_predictions / total_val_samples * 100\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}: \"\n",
        "              f\"Train Loss: {epoch_train_loss:.4f}, Train Acc: {epoch_train_accuracy:.2f}% | \"\n",
        "              f\"Val Loss: {epoch_val_loss:.4f}, Val Acc: {epoch_val_accuracy:.2f}%\")\n",
        "\n",
        "        # --- Early Stopping Logic ---\n",
        "        if epoch_val_loss < best_val_loss:\n",
        "            best_val_loss = epoch_val_loss\n",
        "            best_model_wts = copy.deepcopy(model.state_dict())\n",
        "            epochs_no_improve = 0\n",
        "            # print(f\"  Validation loss improved. Saving model state. Best Loss: {best_val_loss:.4f}\") # Uncomment for more verbose output\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "            # print(f\"  Validation loss did not improve. Patience: {epochs_no_improve}/{patience}\") # Uncomment for more verbose output\n",
        "            if epochs_no_improve >= patience:\n",
        "                print(f\"Early stopping triggered after {epoch+1} epochs due to no improvement in validation loss.\")\n",
        "                model.load_state_dict(best_model_wts)\n",
        "                return model\n",
        "\n",
        "    print(\"Training finished (max epochs reached).\")\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model\n",
        "\n",
        "# --- 7. Evaluation Function ---\n",
        "def evaluate_model(model, data_loader, device, dataset_name=\"Test\"):\n",
        "    \"\"\"\n",
        "    Evaluates the model's performance on a given DataLoader.\n",
        "    \"\"\"\n",
        "    model.eval() # Set model to evaluation mode\n",
        "    all_labels = []\n",
        "    all_predictions = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in data_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "\n",
        "    accuracy = accuracy_score(all_labels, all_predictions)\n",
        "    precision = precision_score(all_labels, all_predictions, average='binary') # 'binary' for 2 classes\n",
        "    recall = recall_score(all_labels, all_predictions, average='binary')\n",
        "    f1 = f1_score(all_labels, all_predictions, average='binary')\n",
        "    cm = confusion_matrix(all_labels, all_predictions)\n",
        "\n",
        "    print(f\"\\n--- {dataset_name} Set Evaluation ---\")\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"F1-Score: {f1:.4f}\")\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(cm)\n",
        "    print(f\"  (Rows: True Labels, Columns: Predicted Labels)\")\n",
        "    print(f\"  [[True Negative (NON-ASD predicted NON-ASD), False Positive (NON-ASD predicted ASD)]\")\n",
        "    print(f\"   [False Negative (ASD predicted NON-ASD), True Positive (ASD predicted ASD)]]\")\n",
        "\n",
        "\n",
        "# --- Main Execution ---\n",
        "if __name__ == \"__main__\":\n",
        "    # 1. Generate and Oversample the Dataset\n",
        "    X_resampled, y_resampled = generate_and_oversample_dataset(\n",
        "        num_samples_total=INITIAL_TOTAL_SAMPLES,\n",
        "        asd_ratio=ASD_RATIO,\n",
        "        image_size=IMAGE_SIZE,\n",
        "        num_electrodes=NUM_ELECTRODES,\n",
        "        sampling_rate=SAMPLING_RATE,\n",
        "        freq_bands=FREQ_BANDS\n",
        "    )\n",
        "\n",
        "    # 2. Prepare DataLoaders for training and validation\n",
        "    train_loader, val_loader = prepare_data_loaders(\n",
        "        X_resampled, y_resampled, BATCH_SIZE, NORM_MEAN, NORM_STD\n",
        "    )\n",
        "\n",
        "    # 3. Get the ResNet-50 model\n",
        "    model = get_resnet_model(NUM_CLASSES, freeze_features=True)\n",
        "\n",
        "    # 4. Define Loss Function and Optimizer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "    # 5. Train the model\n",
        "    trained_model = train_model(\n",
        "        model, train_loader, val_loader, criterion, optimizer, NUM_EPOCHS, PATIENCE, DEVICE\n",
        "    )\n",
        "\n",
        "    print(\"\\n--- Training Complete ---\")\n",
        "\n",
        "    # 6. Evaluate the trained model on the validation set\n",
        "    evaluate_model(trained_model, val_loader, DEVICE, dataset_name=\"Validation\")\n",
        "\n",
        "    # Optional: Visualize a few generated images before training\n",
        "    # This part is included in the eeg_dataset_generator artifact for initial checks.\n",
        "    # You can uncomment and run it here if you want to see samples again.\n",
        "    # print(\"\\nDisplaying a few generated EEG images from the resampled dataset...\")\n",
        "    # fig, axes = plt.subplots(2, 4, figsize=(12, 6))\n",
        "    # axes = axes.flatten()\n",
        "    # for i in range(min(8, len(X_resampled))):\n",
        "    #     ax = axes[i]\n",
        "    #     ax.imshow(X_resampled[i])\n",
        "    #     ax.set_title(f\"Label: {'ASD' if y_resampled[i] == 1 else 'NON-ASD'}\")\n",
        "    #     ax.axis('off')\n",
        "    # plt.tight_layout()\n",
        "    # plt.show()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Generating 1000 initial synthetic EEG images...\n",
            "  Generated 100/1000 samples...\n",
            "  Generated 200/1000 samples...\n",
            "  Generated 300/1000 samples...\n",
            "  Generated 400/1000 samples...\n",
            "  Generated 500/1000 samples...\n",
            "  Generated 600/1000 samples...\n",
            "  Generated 700/1000 samples...\n",
            "  Generated 800/1000 samples...\n",
            "  Generated 900/1000 samples...\n",
            "  Generated 1000/1000 samples...\n",
            "Initial dataset generation complete.\n",
            "Original dataset shape: Counter({np.int64(0): 808, np.int64(1): 192})\n",
            "Applying RandomOverSampler to balance the dataset...\n",
            "Resampled dataset shape: Counter({np.int64(0): 808, np.int64(1): 808})\n",
            "Oversampling complete.\n",
            "\n",
            "Training set distribution after split: Counter({np.int64(0): 646, np.int64(1): 646})\n",
            "Validation set distribution after split: Counter({np.int64(0): 162, np.int64(1): 162})\n",
            "WeightedRandomSampler initialized for training data loaders.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 106MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded pre-trained ResNet-50 model.\n",
            "Frozen all feature extractor layers.\n",
            "Modified final fully connected layer to output 2 classes.\n",
            "\n",
            "Starting model training...\n",
            "Epoch 1/100: Train Loss: 0.7647, Train Acc: 49.46% | Val Loss: 0.7470, Val Acc: 50.00%\n",
            "Epoch 2/100: Train Loss: 0.6888, Train Acc: 54.18% | Val Loss: 0.7099, Val Acc: 52.16%\n",
            "Epoch 3/100: Train Loss: 0.7085, Train Acc: 52.63% | Val Loss: 0.6851, Val Acc: 57.72%\n",
            "Epoch 4/100: Train Loss: 0.6702, Train Acc: 58.28% | Val Loss: 0.6839, Val Acc: 54.63%\n",
            "Epoch 5/100: Train Loss: 0.6299, Train Acc: 67.26% | Val Loss: 0.6726, Val Acc: 59.26%\n",
            "Epoch 6/100: Train Loss: 0.6261, Train Acc: 65.17% | Val Loss: 0.6777, Val Acc: 57.72%\n",
            "Epoch 7/100: Train Loss: 0.6115, Train Acc: 67.65% | Val Loss: 0.6788, Val Acc: 56.17%\n",
            "Epoch 8/100: Train Loss: 0.6059, Train Acc: 67.96% | Val Loss: 0.6711, Val Acc: 59.57%\n",
            "Epoch 9/100: Train Loss: 0.5956, Train Acc: 69.97% | Val Loss: 0.7086, Val Acc: 54.32%\n",
            "Epoch 10/100: Train Loss: 0.5994, Train Acc: 65.48% | Val Loss: 0.6727, Val Acc: 60.80%\n",
            "Epoch 11/100: Train Loss: 0.5932, Train Acc: 70.59% | Val Loss: 0.6679, Val Acc: 60.80%\n",
            "Epoch 12/100: Train Loss: 0.5774, Train Acc: 72.76% | Val Loss: 0.6970, Val Acc: 54.94%\n",
            "Epoch 13/100: Train Loss: 0.5859, Train Acc: 70.43% | Val Loss: 0.6615, Val Acc: 60.19%\n",
            "Epoch 14/100: Train Loss: 0.5638, Train Acc: 72.06% | Val Loss: 0.6718, Val Acc: 57.72%\n",
            "Epoch 15/100: Train Loss: 0.5361, Train Acc: 76.16% | Val Loss: 0.6850, Val Acc: 58.64%\n",
            "Epoch 16/100: Train Loss: 0.5449, Train Acc: 75.54% | Val Loss: 0.6633, Val Acc: 60.80%\n",
            "Epoch 17/100: Train Loss: 0.5546, Train Acc: 73.84% | Val Loss: 0.6729, Val Acc: 58.64%\n",
            "Epoch 18/100: Train Loss: 0.5477, Train Acc: 73.45% | Val Loss: 0.6703, Val Acc: 60.80%\n",
            "Epoch 19/100: Train Loss: 0.5412, Train Acc: 75.54% | Val Loss: 0.6652, Val Acc: 60.49%\n",
            "Epoch 20/100: Train Loss: 0.5391, Train Acc: 74.69% | Val Loss: 0.6579, Val Acc: 62.04%\n",
            "Epoch 21/100: Train Loss: 0.5483, Train Acc: 73.68% | Val Loss: 0.6700, Val Acc: 65.74%\n",
            "Epoch 22/100: Train Loss: 0.5377, Train Acc: 74.92% | Val Loss: 0.6544, Val Acc: 65.43%\n",
            "Epoch 23/100: Train Loss: 0.5361, Train Acc: 75.54% | Val Loss: 0.6498, Val Acc: 62.35%\n",
            "Epoch 24/100: Train Loss: 0.5238, Train Acc: 76.16% | Val Loss: 0.6508, Val Acc: 64.20%\n",
            "Epoch 25/100: Train Loss: 0.5209, Train Acc: 76.86% | Val Loss: 0.6505, Val Acc: 62.96%\n",
            "Epoch 26/100: Train Loss: 0.5146, Train Acc: 75.70% | Val Loss: 0.6558, Val Acc: 61.73%\n",
            "Epoch 27/100: Train Loss: 0.5163, Train Acc: 75.62% | Val Loss: 0.6432, Val Acc: 63.89%\n",
            "Epoch 28/100: Train Loss: 0.5081, Train Acc: 78.33% | Val Loss: 0.6333, Val Acc: 65.12%\n",
            "Epoch 29/100: Train Loss: 0.5228, Train Acc: 74.92% | Val Loss: 0.6330, Val Acc: 67.28%\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1674783827.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m     \u001b[0;31m# 5. Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 418\u001b[0;31m     trained_model = train_model(\n\u001b[0m\u001b[1;32m    419\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPATIENCE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEVICE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m     )\n",
            "\u001b[0;32m/tmp/ipython-input-1674783827.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, criterion, optimizer, num_epochs, patience, device)\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36m_forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    274\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavgpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 554\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    555\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    547\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m             )\n\u001b[0;32m--> 549\u001b[0;31m         return F.conv2d(\n\u001b[0m\u001b[1;32m    550\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m         )\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "execution_count": null,
      "metadata": {
        "id": "ZhLs0oV3x50i",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a6be5cb8-095f-4267-9c5d-ec6201299114"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}